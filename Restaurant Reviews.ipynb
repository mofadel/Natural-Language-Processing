{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Restaurant Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, we are going to do analysis of Restaurant reviews: positive or negative reviews, using NLP techniques.\n",
    "The dataset we are using contains 1000 samples with their corresponding labels: 0 for negative review and 1 for positive review.\n",
    "These are the different steps in our work:\n",
    "\n",
    "\n",
    "        1- Data Preprocessing\n",
    "        2- Split the dataset into train and test\n",
    "        3- Fit a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import some libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the dataset\n",
    "dataset = pd.read_csv(\"Restaurant_Reviews.tsv\", delimiter=\"\\t\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the repartition of the dataset, how many 1 and 0? missing data?\n",
    "This might be very useful so that we can know which kind one preprocessing we need to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews:  500\n",
      "Number of the negative reviews:  500\n",
      "Missing data:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Number of positive reviews: \",sum(dataset[\"Liked\"]==1))\n",
    "print(\"Number of the negative reviews: \", sum(dataset[\"Liked\"]==0))\n",
    "print(\"Missing data: \", dataset.isnull().sum().sum())\n",
    "\n",
    "#plt.figure(figsize=[10,7])\n",
    "#plt.subplot(1,3,1)\n",
    "#dataset[\"Liked\"].value_counts().plot.pie()\n",
    "dataset.hist()\n",
    "#plt.subplot(1,4,1)\n",
    "#dataset[\"Review\"].value_counts().plot.pie()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will set all the words into lower case, clean the data by removing the punctuations and the numbers and transform all reviews words by lemmatization technique.\n",
    "Why is this preprocessing important?\n",
    "\n",
    "    a- We set all the words to lower case so that we will not have a distinction between, \"NICE\" and \"nice\" for instance\n",
    "    \n",
    "    b- Punctuations and numbers are not needed here because they don't discriminate the data\n",
    "    \n",
    "    c- Lemmatization is a technique called \"Word Normalization\": it is a way to truncate words to keep the canonical form or the dictionary form. Then all the words in the dataset having the same normalization will share the same feature. This technique is similar to Stemming but the difference is Stemming keep only the root of the word. For instance, if we have the sentence \"We use scientific methods in data science\", the output after stemming would be \" we use scientif method in data scienc\", and the output after lemmatization will be \"we use scientifique method in data science.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "#nltk.download() #decomment it if you have not installed yet this corpus\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Lemmatization\n",
      "We\n",
      "use\n",
      "scientific\n",
      "method\n",
      "data\n",
      "science\n",
      "\t Stemming\n",
      "We\n",
      "use\n",
      "scientif\n",
      "method\n",
      "data\n",
      "scienc\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t Lemmatization\")\n",
    "l = WordNetLemmatizer()\n",
    "s = \"We use scientific methods in data science \"\n",
    "for word in s.split():\n",
    "    if not word in set(stopwords.words('english')):\n",
    "        print(l.lemmatize(word))\n",
    "print(\"\\t Stemming\")\n",
    "ps = PorterStemmer()\n",
    "for word in s.split():\n",
    "    if not word in set(stopwords.words('english')):\n",
    "        print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize empty array \n",
    "# to append clean text  \n",
    "corpus = []  \n",
    "# 1000 (reviews) rows to clean \n",
    "for i in range(0, 1000):      \n",
    "    # column : \"Review\", row ith \n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])  # it replaces everything that it not alphabetic character by a space\n",
    "    review = review.lower()  # convert all cases to lower cases    \n",
    "    review = review.split()  # split to array(default delimiter is \" \")     \n",
    "    # creating Lemmatizer object to \n",
    "    # take main stem of each word \n",
    "    lemmatizer = WordNetLemmatizer()     \n",
    "    # loop for lemmatization each word \n",
    "    # in string array at ith row     \n",
    "    review = [lemmatizer.lemmatize(word) for word in review \n",
    "                if not word in set(stopwords.words('english'))]                  \n",
    "    # rejoin all string array elements \n",
    "    # to create back into a string \n",
    "    rev = ' '.join(review)       \n",
    "    # append each string to create \n",
    "    # array of clean text  \n",
    "    corpus.append(rev)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow loved place'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can visualize some of the transformation done on the dataset by the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t Initial Dataset\n",
      "0                             Wow... Loved this place.\n",
      "1                                   Crust is not good.\n",
      "2            Not tasty and the texture was just nasty.\n",
      "3    Stopped by during the late May bank holiday of...\n",
      "4    The selection on the menu was great and so wer...\n",
      "Name: Review, dtype: object \n",
      "\n",
      "\t\t After preprocessing\n",
      "['wow loved place', 'crust good', 'tasty texture nasty', 'stopped late may bank holiday rick steve recommendation loved', 'selection menu great price']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\t Initial Dataset\")\n",
    "print(dataset['Review'][:5],\"\\n\")\n",
    "print(\"\\t\\t After preprocessing\")\n",
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to create our feature matrix where every review will be represented as a sparse vector: this is a Bag Of Words (BOW) representations. For that task, we need CountVectorizer from sklearn.feature_extraction.text.\n",
    "In this CountVectorizer class we can set the maximum number of features (max no. features which help the most via attribute “max_features”). Do the training on corpus and then apply the same transformation to the corpus “.fit_transform(corpus)” and then convert it into array. If review is positive or negative that answer is in second column of : dataset[:, 1] : all rows ans 1st column (indexing from zero).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(max_features = 1500)\n",
    "word2vec = count_vect.fit_transform(corpus).toarray() #create a matrix where all the reviews(after transformation) are stored with their vector representation\n",
    "true_labels = dataset.iloc[:,1] # get all the labels from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.shape\n",
    "word2vec[0]\n",
    "#print(sum(word2vec[0]==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Training and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is the only dataset available, we will split it into two parts: a training part and a testing part using train_test_split from sklearn.cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, x_test, Y_train, y_test = train_test_split(word2vec, true_labels, test_size = .3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Fitting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are doing binary classification, we can fit a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.793333333333\n",
      "Confusion Matrix:  [[126  26]\n",
      " [ 36 112]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "clf = LogisticRegression(solver = 'lbfgs')\n",
    "clf.fit(X_train, Y_train)\n",
    "pred_labels = clf.predict(x_test)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, pred_labels))\n",
    "print(\"Confusion Matrix: \", confusion_matrix(y_test, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
